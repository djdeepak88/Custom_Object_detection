{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52e04cb-f8a7-4500-b9ef-06727e4b820e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://artifacts.jpmchase.net/artifactory/api/pypi/pypi/simple/\n",
      "Requirement already satisfied: torch in /opt/omniai/software/Miniconda/lib/python3.8/site-packages (2.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/omniai/software/Miniconda/lib/python3.8/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/omniai/software/Miniconda/lib/python3.8/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /opt/omniai/software/Miniconda/lib/python3.8/site-packages (from torch) (2.18.1)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/omniai/software/Miniconda/lib/python3.8/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/omniai/software/Miniconda/lib/python3.8/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/omniai/software/Miniconda/lib/python3.8/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: networkx in /opt/omniai/software/Miniconda/lib/python3.8/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/omniai/software/Miniconda/lib/python3.8/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: filelock in /opt/omniai/software/Miniconda/lib/python3.8/site-packages (from torch) (3.14.0)\n",
      "Requirement already satisfied: jinja2 in /opt/omniai/software/Miniconda/lib/python3.8/site-packages (from torch) (3.0.1)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/omniai/software/Miniconda/lib/python3.8/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/omniai/software/Miniconda/lib/python3.8/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: sympy in /opt/omniai/software/Miniconda/lib/python3.8/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/omniai/software/Miniconda/lib/python3.8/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: fsspec in /opt/omniai/software/Miniconda/lib/python3.8/site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions in /opt/omniai/software/Miniconda/lib/python3.8/site-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /opt/omniai/software/Miniconda/lib/python3.8/site-packages (from torch) (2.1.0)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/omniai/software/Miniconda/lib/python3.8/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/omniai/software/Miniconda/lib/python3.8/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.3.101)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/omniai/software/Miniconda/lib/python3.8/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/omniai/software/Miniconda/lib/python3.8/site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install torchvision\n",
    "!pip install pycocotools\n",
    "!pip install xmltodict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23dbfc91-abc1-4711-b122-3af2627258ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import pycocotools\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import v2\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import xmltodict\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a451d21-5972-46a5-9bb4-a6b1afbe6006",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.memory_summary(device=None, abbreviated=False)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.device(0))\n",
    "print(torch.cuda.get_device_name(0))\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print(torch.cuda.get_device_name(1))\n",
    "    print(torch.cuda.get_device_name(2))\n",
    "    print(torch.cuda.get_device_name(3))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:- ', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:-   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7c17ca-9a92-48cb-bb72-1b33b03cf469",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_names=[] \n",
    "xml_names=[] \n",
    "for dirname, _, filenames in os.walk('MaskedFace/train'):\n",
    "    for filename in filenames:\n",
    "        if os.path.join(dirname, filename)[-3:]!=\"xml\":\n",
    "            img_names.append(filename)\n",
    "        else:\n",
    "            xml_names.append(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c97192-f94f-4bda-a15e-11c2af203010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image sizes\n",
    "#img1 = Image.open(\"train/mask-000.png\").convert('RGB')\n",
    "#print(img1.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5910ecd6-41ba-4a95-a62a-682bd4fed3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_annotations=\"MaskedFace/train/\" \n",
    "listing=[]\n",
    "img_width=[]\n",
    "img_height=[]\n",
    "for i in img_names[:]:\n",
    "    #print(i)\n",
    "    with open(path_annotations+i[:-4]+\".xml\") as fd:\n",
    "        doc=xmltodict.parse(fd.read())\n",
    "    \n",
    "    Image_sizes = doc[\"annotation\"][\"size\"]\n",
    "    \n",
    "    img_width.append(Image_sizes[\"width\"])\n",
    "    img_height.append(Image_sizes[\"height\"])\n",
    "    \n",
    "    temp=doc[\"annotation\"][\"object\"]\n",
    "    if type(temp)==list:\n",
    "        for i in range(len(temp)):\n",
    "            listing.append(temp[i][\"name\"])\n",
    "    else:\n",
    "        listing.append(temp[\"name\"])\n",
    "        \n",
    "\n",
    "Items = Counter(listing).keys()\n",
    "values = Counter(listing).values()\n",
    "print(Items,'\\n',values)\n",
    "\n",
    "print(\"Image Sizes:-\")\n",
    "print(\"Minimum Width:-\")\n",
    "print(min(img_width))\n",
    "print(\"Minimum Height:-\")\n",
    "print(min(img_height))\n",
    "\n",
    "print(\"Mean Sizes of Images:-\")\n",
    "print(statistics.mean(map(int,img_width)))\n",
    "print(statistics.mean(map(int,img_height)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dc62fe-e24d-4b9c-b125-0012d7845259",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize =(14,6))\n",
    "background_color = '#faf9f4'\n",
    "ax1.set_facecolor(background_color)\n",
    "ax2.set_facecolor(background_color) \n",
    "ax1.pie(values,wedgeprops=dict(width=0.3, edgecolor='w') ,\n",
    "        labels=Items, radius=1, startangle = 120, autopct='%1.2f%%')\n",
    "\n",
    "ax2 = plt.bar(Items, list(values),\n",
    "              color ='maroon',width = 0.4)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06be67ce-9339-43a0-87b4-a5e778bfd433",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_str2num = {'with_mask': 1,'without_mask': 2,'mask_weared_incorrect': 3}\n",
    "class_num2str = {v: k for k, v in class_str2num.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb06106-2cb6-467d-96d5-feeb97360903",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_str2num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641a0a7f-e157-422a-9c1c-88f9e06b048e",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_list = sorted(os.listdir('MaskedFace/annotations'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211fd9e5-2090-4d3c-b053-7358bce233ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db226abe-3a84-4235-8a53-bbcdb737dbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "def parse_xml(xml_file):\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "    bboxes = []\n",
    "    labels = []\n",
    "    filename = root.find('filename').text\n",
    "    for boxes in root.iter('object'):\n",
    "        ymin, xmin, ymax, xmax = None, None, None, None\n",
    "        ymin = int(boxes.find(\"bndbox/ymin\").text)\n",
    "        xmin = int(boxes.find(\"bndbox/xmin\").text)\n",
    "        ymax = int(boxes.find(\"bndbox/ymax\").text)\n",
    "        xmax = int(boxes.find(\"bndbox/xmax\").text)\n",
    "        box = [xmin, ymin, xmax, ymax]\n",
    "        bboxes.append(box)\n",
    "        labels.append(int(class_str2num[boxes.find(\"name\").text]))\n",
    "    return filename, bboxes, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9429a7-13db-45e0-8d8a-fde701ca4c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "idx = np.random.randint(len(annotations_list))\n",
    "print(\"Index\")\n",
    "print(idx)\n",
    "filename, boxes, labels = parse_xml(os.path.join('MaskedFace/train', annotations_list[idx]))\n",
    "print(filename)\n",
    "#print(boxes)\n",
    "print(labels)\n",
    "image = Image.open(os.path.join('MaskedFace/train', filename))\n",
    "#print the bounding box on the image.\n",
    "draw = ImageDraw.Draw(image)\n",
    "for i, ibox in enumerate(boxes):\n",
    "    draw.rectangle([(ibox[0], ibox[1]), (ibox[2], ibox[3])], outline='red', width=3)\n",
    "    draw.text((ibox[0], ibox[1]), text=class_num2str[labels[i]])\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce74a7be-4494-4918-a1c8-8552fe780940",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transforms=None):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        self.annotations_list = sorted(os.listdir(os.path.join(root, 'annotations')))\n",
    "\n",
    "    def __getitem__(self, indx):\n",
    "        \n",
    "        # load annotation\n",
    "        filename, boxes, labels = parse_xml(os.path.join(self.root, 'annotations', self.annotations_list[indx]))\n",
    "        \n",
    "        # load image\n",
    "        img_path = os.path.join(self.root, 'train', filename)\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        num_objs = boxes.shape[0]\n",
    "        \n",
    "        #classes\n",
    "        labels = torch.tensor(labels, dtype=torch.int64)\n",
    "        image_id = torch.tensor([indx])\n",
    "        \n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "        \n",
    "        target = {}\n",
    "        target['boxes'] = boxes\n",
    "        target['labels'] = labels\n",
    "        target['image_id'] = image_id\n",
    "        target['area'] = area\n",
    "        target['iscrowd'] =iscrowd\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "        return img, target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.annotations_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373a0e33-72fb-4dde-b44d-9084b63af0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MaskedDataset(root='MaskedFace')\n",
    "img, target = dataset.__getitem__(1)\n",
    "\n",
    "boxes={}\n",
    "for k,v in target.items():\n",
    "    print(k)\n",
    "    #print(v)\n",
    "    print(target['labels'])\n",
    "    boxes = target['boxes']  \n",
    "\n",
    "print(\"Boxes:-\")\n",
    "print(boxes)\n",
    "        \n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0c1ff5-9b57-483f-ad1d-a320c4fe1302",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_from_file(num_classes, flag=True):   \n",
    "    # Load an object detection model pretrained on MS COCO.\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=flag, weights_backbone=flag)\n",
    "    # Load the fasterrcnn resnet50.\n",
    "    #chk = torch.load('fasterrcnn_resnet50_fpn_coco-258fb6c6.pth')\n",
    "    # Load state dictionary.\n",
    "    #model.load_state_dict(chk)\n",
    "    # get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new on\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    # return model.\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92aeb77-6e5a-4a71-b06c-a877873625d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "image_transforms = {\n",
    "    # Train uses data augmentation\n",
    "    'train':\n",
    "    transforms.Compose([\n",
    "        transforms.Resize(size=(224,224)),\n",
    "        transforms.RandomRotation(degrees=15),\n",
    "        transforms.ColorJitter(),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])  # Imagenet standards\n",
    "    ]),\n",
    "    # Validation does not use augmentation\n",
    "    'test':\n",
    "    transforms.Compose([\n",
    "        transforms.Resize(size=(224,224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], \n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c72cbe-d863-4591-86cd-d6b5508337fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform(train):\n",
    "    #Transform of the images.\n",
    "    transform_list= []\n",
    "    \n",
    "    transform_list.append(v2.PILToTensor())\n",
    "    transform_list.append(v2.ConvertImageDtype(torch.float))\n",
    "    \n",
    "    if train:\n",
    "        # during training, randomly flip the training images\n",
    "        # and ground-truth for data augmentation\n",
    "        # This configuration best for training.\n",
    "        # transform_list.append(v2.Resize((256,256), antialias=None))\n",
    "        # transform_list.append(v2.RandomResizedCrop(size=(224, 224), antialias=True))\n",
    "        # transform_list.append(v2.ColorJitter())\n",
    "        transform_list.append(v2.RandomHorizontalFlip(0.5))\n",
    "        # Random affine not working well at lower epochs.\n",
    "        # transform_list.append(v2.RandomAffine(15))   \n",
    "        # Image resize transformation does not work well for this training.\n",
    "        # transform_list.append(v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]))\n",
    "        # transform_list.append(v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]))\n",
    "    # else:\n",
    "        # transform_list.append(v2.Resize((256,256), antialias=None))\n",
    "        # transform_list.append(v2.RandomResizedCrop(size=(224, 224), antialias=True))\n",
    "        # transform_list.append(v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]))\n",
    "        # transform_list.append(v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]))\n",
    "        \n",
    "    transform_object = v2.Compose(transform_list)\n",
    "    \n",
    "    return transform_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf92dcc-0866-4bce-a35d-78b145ae1cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def get_transform(train):\n",
    "    #Transform of the images.\n",
    "    transform_list= []\n",
    "    transform_list.append(v2.PILToTensor())\n",
    "    transform_list.append(v2.ConvertImageDtype(torch.float))\n",
    "    \n",
    "    if train:\n",
    "        # during training, randomly flip the training images\n",
    "        # and ground-truth for data augmentation\n",
    "        # This configuration best for training.\n",
    "        transform_list.append(v2.RandomHorizontalFlip(0.5))\n",
    "        # Random affine not working well at lower epochs.\n",
    "        # transform_list.append(v2.RandomAffine(5))   \n",
    "        # Image resize transformation does not work well for this training.\n",
    "        # transform_list.append(v2.Normalize((0.5, 0.5, 0.5),(0.5, 0.5, 0.5)))\n",
    "        \n",
    "    transform_object = v2.Compose(transform_list)\n",
    "    \n",
    "    return transform_object\n",
    "'''  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed83a4e0-846a-43c1-b650-26311b8ef349",
   "metadata": {},
   "outputs": [],
   "source": [
    "Tobj = get_transform(train=True)\n",
    "Tobj1 = get_transform(train=False)\n",
    "print(Tobj)\n",
    "print(Tobj1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49b371b-2bb6-4042-9730-91209d4c1d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcceeb38-6624-4e82-be06-5a91f35c37d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MaskedDataset(root= 'MaskedFace', transforms = get_transform(train=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bef055-a2c0-45d3-8766-ac31c7adf38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "\n",
    "cpu_count = mp.cpu_count()\n",
    "\n",
    "print(cpu_count-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f886a62-613b-46cd-947b-070d821bec41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import multiprocessing as mp\n",
    "\n",
    "# Checking the optimal num of workers for loading the training dataset.\n",
    "# for num_workers in range(2, mp.cpu_count(), 2):  \n",
    "    #train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=num_workers, collate_fn=collate_fn)\n",
    "    #start = time()\n",
    "    #for epoch in range(1, 3):\n",
    "        #for i, data in enumerate(train_loader, 0):\n",
    "        #    pass\n",
    "    #end = time()\n",
    "    #print(\"Finish with:{} second, num_workers={}\".format(end - start, num_workers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb53b80c-836d-4700-8bed-a0015613d373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataset = MaskedDataset(root='.', transforms=get_transform(train=False))\n",
    "# dataset\n",
    "#indices = torch.randperm(len(train_dataset)).tolist()\n",
    "# A subset of the above original datasets are used for quick training and\n",
    "# testing iterations\n",
    "#train_dataset = Subset(train_dataset, indices[:-10])\n",
    "#test_dataset = Subset(test_dataset, indices[-10:])\n",
    "# data loader\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=18, collate_fn=collate_fn)\n",
    "#test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False, num_workers=2, collate_fn=collate_fn)\n",
    "#print(\"We have: {} examples, {} are training and {} testing\".format(len(indices), len(train_dataset), len(test_dataset)))\n",
    "print(\"We have: {} are training\".format(len(train_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2459ac6-e2c2-4797-a012-51c6ec5032a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)\n",
    "num_classes = len(class_str2num) + 1\n",
    "print(\"Classes:-\")\n",
    "print(num_classes)\n",
    "# get the model using our helper function\n",
    "model = get_model_from_file(num_classes)\n",
    "# print(model)\n",
    "# move model to the GPU Device.\n",
    "model.to(device)\n",
    "print(\"Model Loaded to : {}\".format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4b9e59-c51b-43bc-b715-5b4d0d110a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct an optimizer\n",
    "#params = [p for p in model.parameters() if p.requires_grad]\n",
    "#optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "# and a learning rate scheduler which decreases the learning rate by # 10x every 3 epochs\n",
    "#lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "### This works best. The best combination of the training and optimization.\n",
    "#from engine import train_one_epoch, evaluate\n",
    "#import utils\n",
    "# let's train it for 10 epochs\n",
    "#num_epochs = 1000\n",
    "#for epoch in range(num_epochs):\n",
    "    # train for one epoch, printing every 10 iterations\n",
    "    #train_one_epoch(model, optimizer, train_loader, device, epoch, print_freq=10)\n",
    "    # evaluate on the test dataset\n",
    "    #evaluate(model, test_loader, device=device)\n",
    "    # update the learning rate\n",
    "    #lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405beffb-56ac-4af8-950b-78be69e9a8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Epoch Counts.\n",
    "num_epochs = 500\n",
    "# Parameters.\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "# Optimizer.\n",
    "# Adding the nesterov momentum variable.\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "# Adam optimizer not working well in this scenario.\n",
    "# optimizer = torch.optim.Adam(params, lr=0.005, weight_decay=0.0005)\n",
    "# length of training dataloader.\n",
    "len_dataloader = len(train_loader)\n",
    "\n",
    "# LR Scheduler of the optimizer.\n",
    "# StepLR is not converging well.\n",
    "# lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
    "# Linear LR is converging well. for warmup factor 0.01 and warmup_iters = min(100,len(train_loader)-1)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "# losses for each epochs.\n",
    "ep_loss_list=[]\n",
    "# Linear LR is also coverging well.\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # Model to be in training mode.\n",
    "    model.train()\n",
    "    \n",
    "    i = 0 \n",
    "    epoch_loss = 0\n",
    "    \n",
    "    lr_scheduler_inter_epoch = None\n",
    "    \n",
    "    if epoch == 0:\n",
    "        warmup_factor = 1.0 / 1000\n",
    "        warmup_iters = min(1000, len(train_loader) - 1)\n",
    "\n",
    "        lr_scheduler_inter_epoch = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=warmup_factor, total_iters=warmup_iters)\n",
    "    \n",
    "    # iterate on the train_loader.\n",
    "    for imgs, targets in train_loader:\n",
    "        i += 1\n",
    "        imgs = list(img.to(device) for img in imgs)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        \n",
    "        loss_dict = model(imgs,targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())        \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step() \n",
    "        epoch_loss += losses\n",
    "        \n",
    "        if lr_scheduler_inter_epoch is not None:\n",
    "            lr_scheduler_inter_epoch.step()\n",
    "        \n",
    "    # StepLR for lr_scheduler.\n",
    "    lr_scheduler.step()\n",
    "    \n",
    "    # Save the model checkpoint after every 10 epochs.\n",
    "    e_loss =  np.round(epoch_loss.detach().cpu().numpy(), decimals=2)\n",
    "    ep_loss_val = round(e_loss)\n",
    "    #print(\"E Loss\")\n",
    "    #print(ep_loss_val)\n",
    "    #print(type(ep_loss_val))\n",
    "    ep_loss_list.append(ep_loss_val)\n",
    "    \n",
    "    print(f'Epoch:{epoch}, Epoch_Loss: {epoch_loss}')\n",
    "    \n",
    "    # Save the lowest loss model state.\n",
    "    if ep_loss_val <= min(ep_loss_list):\n",
    "        torch.save(model.state_dict(),'trained_model_faster_rcnn_epoch_{}_loss_{}.pth'.format(epoch,ep_loss_val))\n",
    "    \n",
    "    # print in the intervals of 100.\n",
    "    if (epoch % 300) == 0:\n",
    "        print(f'Epoch:{epoch}, Epoch_Loss: {epoch_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be7e0d7-9946-4756-8ba8-0d4f4d0401d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put Flag=\"True\" for downloading the model and not using the \n",
    "# downloaded model.\n",
    "def load_model_from_file(num_classes, filename, flag=True):   \n",
    "    # Load an object detection model pretrained on MS COCO.\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=flag, weights_backbone=flag)\n",
    "    # Load the fasterrcnn resnet50.\n",
    "    chk = torch.load(filename)\n",
    "    # Load state dictionary.\n",
    "    # get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new on\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    # return model.\n",
    "    model.load_state_dict(chk)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4713dee-0902-47af-963d-7e7577f322ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename = 'trained_model_faster_rcnn_epoch_730_loss_66.44000244140625.pth'\n",
    "loaded_model = load_model_from_file(num_classes, model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f0b3e2-5511-4673-9d49-d4160ee01ec9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_dataset = MaskedDataset(root='test_imgs', transforms=get_transform(train=False))\n",
    "print(len(val_dataset))\n",
    "# data loader\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=True, num_workers=12, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c977eb1-6d79-4bc7-9a78-7e87d7115148",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "infer_model = loaded_model.cpu()\n",
    "infer_model.eval()\n",
    "print(\"Evaluation Mode of Model:-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfdd994-b914-4c3b-a81d-1bb07f8db9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "print(\"Class numbers\")\n",
    "print(class_num2str)\n",
    "\n",
    "def calculate_MAPE(test_dataset, confidence_score, model):\n",
    "    \n",
    "    class_score = 0\n",
    "    class_score_ind=[]\n",
    "    \n",
    "    for i in range(len(test_dataset)):\n",
    "        \n",
    "        img, _ = test_dataset[i]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Prediction for the particular image.\n",
    "            prediction = model([img])\n",
    "        \n",
    "        groundtruth_labels = test_dataset[i][1]['labels'].numpy()\n",
    "        confident_prediction_labels=[]\n",
    "        \n",
    "        # For all the predicted boxes check the labels and confidence scores.\n",
    "        for element in range(len(prediction[0]['boxes'])):\n",
    "\n",
    "            #print(\"Prediction Label of each box:-\")\n",
    "            ele_pred_label = prediction[0]['labels'][element].cpu().item()\n",
    "            #print(ele_pred_label)\n",
    "            \n",
    "            score = np.round(prediction[0]['scores'][element].cpu().numpy(), decimals= 4)\n",
    "            # if the score is greater than confidence score.\n",
    "            if score > confidence_score:\n",
    "               # print(score)\n",
    "               # if  the score is greater than the confidence score\n",
    "               confident_prediction_labels.append(ele_pred_label)\n",
    "        \n",
    "        #print(\"The confident prediction labels:-\")\n",
    "        #print(confident_prediction_labels)\n",
    "        C_grd = Counter(groundtruth_labels)\n",
    "        #print(C_grd)\n",
    "        C_pred = Counter(confident_prediction_labels)   \n",
    "        #print(C_pred)\n",
    "        \n",
    "        # Classes in the Object detection:-\n",
    "        # print(class_num2str)\n",
    "        class_score=0\n",
    "        # Class keys.\n",
    "        for k in class_num2str.keys():\n",
    "            # print(\"Class Labels:-\")\n",
    "            # print(k)\n",
    "            At = C_grd[k]\n",
    "            Pt = C_pred[k] \n",
    "            class_score += abs(At-Pt)/max(At,1)\n",
    "            \n",
    "        class_score_ind.append(class_score)\n",
    "    \n",
    "    sum_scores = sum(class_score_ind)\n",
    "    print(\"Scores_sum\")\n",
    "    print(sum_scores)\n",
    "    #print(\"Class Scores:-\")\n",
    "    #print(class_score)  \n",
    "    #print(\"Class_score_List:-\")\n",
    "    #print(class_score_ind)\n",
    "    #print(\"length of class scores:-\")\n",
    "    #print(len(class_score_ind))\n",
    "    #print(\"length of test dataset\")\n",
    "    #print(len(test_dataset))\n",
    "    MAPE = sum_scores / (len(class_num2str.keys()))\n",
    "    Avg_MAPE = (MAPE/len(test_dataset)) * 100\n",
    "    print(\"Avg MAPE Score on the dataset\")\n",
    "    print(Avg_MAPE)\n",
    "    \n",
    "confidence_score = 0.4\n",
    "calculate_MAPE(val_dataset, confidence_score, infer_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb933d4-b7bb-4552-a189-e2e9fd31f2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count masked faces\n",
    "def count_masks(test_dataset, model):\n",
    "    \n",
    "    len_dataset = len(test_dataset)\n",
    "    \n",
    "    inference_results_matrix = np.zeros((len_dataset,3))\n",
    "\n",
    "    for i in range(len(val_dataset)):\n",
    "        img, _ = val_dataset[i]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            prediction = model([img])\n",
    "    \n",
    "        for element in range(len(prediction[0]['boxes'])):\n",
    "            label = prediction[0]['labels'][element].cpu().item()\n",
    "            score = np.round(prediction[0]['scores'][element].cpu().numpy(), decimals= 4)\n",
    "            if score > 0.4:\n",
    "               inference_results_matrix[i][label-1] += 1 \n",
    "            \n",
    "    return inference_results_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee8fa85-e2f4-45d8-a9ea-34b87cf1f11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "inf_results = count_masks(val_dataset, infer_model)\n",
    "print(inf_results.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78942b94-1ba4-4115-b834-99776fc4b4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "inf_results[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efb4454-01b2-4227-ad34-b9cd1be7c034",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (10.0, 10.0)\n",
    "plt.rcParams['figure.dpi'] = 72\n",
    "model = model.cpu()\n",
    "model.eval()\n",
    "for i in range(len(val_dataset)):\n",
    "    img, _ = val_dataset[i]\n",
    "    label_boxes = np.array(val_dataset[i][1]['boxes'])\n",
    "    #put the model in evaluation mode\n",
    "    with torch.no_grad():\n",
    "        prediction = model([img])\n",
    "    image = Image.fromarray(img.mul(255).permute(1, 2, 0).byte().numpy())\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    # draw groundtruth\n",
    "    for elem in range(len(label_boxes)):\n",
    "        draw.rectangle([(label_boxes[elem][0], label_boxes[elem][1]), (label_boxes[elem][2], label_boxes[elem][3])], outline =\"yellow\", width =3)\n",
    "    for element in range(len(prediction[0]['boxes'])):\n",
    "        box = prediction[0]['boxes'][element].cpu().numpy()\n",
    "        label = prediction[0]['labels'][element].cpu().item()\n",
    "        score = np.round(prediction[0]['scores'][element].cpu().numpy(), decimals= 4)\n",
    "        if score > 0.4:\n",
    "            draw.rectangle([(box[0], box[1]), (box[2], box[3])], outline ='blue', width =3)\n",
    "            draw.text((box[0], box[1]), text = class_num2str[label] + ', ' + str(score))\n",
    "    plt.imshow(image)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
