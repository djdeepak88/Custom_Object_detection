{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "552b6094-33ea-4d7f-9512-1967c0c7fcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import pycocotools\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import v2\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import xmltodict\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "680b6963-83ef-4ea7-96d1-5e37d6d806bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd3a17a3-d76f-49c7-bc89-b5c40e276e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_from_file(num_classes, filename, flag=True):   \n",
    "    # Load an object detection model pretrained on MS COCO.\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=flag, weights_backbone=flag)\n",
    "    # Load the fasterrcnn resnet50.\n",
    "    chk = torch.load(filename)\n",
    "    # Load state dictionary.\n",
    "    # get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new on\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    # return model.\n",
    "    model.load_state_dict(chk)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad96629e-7335-4e88-b2df-0cf75d595a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/omniai/software/Miniconda/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "/opt/omniai/software/Miniconda/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights_backbone' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights_backbone=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights_backbone=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'trained_model_faster_rcnn_epoch_464_loss_24.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_34870/1774931508.py\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mmodel_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'trained_model_faster_rcnn_epoch_464_loss_24.pth'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mloaded_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_34870/2882103059.py\u001b[0m in \u001b[0;36mload_model_from_file\u001b[0;34m(num_classes, filename, flag)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfasterrcnn_resnet50_fpn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflag\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_backbone\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# Load the fasterrcnn resnet50.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mchk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Load state dictionary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# get the number of input features for the classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/omniai/software/Miniconda/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    984\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/omniai/software/Miniconda/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/omniai/software/Miniconda/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'trained_model_faster_rcnn_epoch_464_loss_24.pth'"
     ]
    }
   ],
   "source": [
    "num_classes = 4\n",
    "# model_filename = 'trained_model_faster_rcnn_epoch_370_loss_60.pth'\n",
    "# model_filename = 'trained_model_faster_rcnn_epoch_10_loss_58.pth' # 8.5\n",
    "# model_filename = 'trained_model_faster_rcnn_epoch_40_loss_61.pth' # 9.06\n",
    "# model_filename = 'trained_model_faster_rcnn_epoch_60_loss_62.pth' # 9.69\n",
    "# model_filename = 'trained_model_faster_rcnn_epoch_70_loss_61.pth' # 9.59\n",
    "# model_filename = 'trained_model_faster_rcnn_epoch_90_loss_63.pth' # 9.61\n",
    "# model_filename = 'trained_model_faster_rcnn_epoch_100_loss_61.pth' # 9.5\n",
    "# model_filename = 'trained_model_faster_rcnn_epoch_120_loss_63.pth' # 9.5\n",
    "# model_filename = 'trained_model_faster_rcnn_epoch_10_loss_121.pth' # \n",
    "# model_filename = 'trained_model_faster_rcnn_epoch_48_loss_69.pth' # 8.5\n",
    "# model_filename = 'trained_model_faster_rcnn_epoch_245_loss_68_Final.pth'\n",
    "\n",
    "model_filename = 'trained_model_faster_rcnn_epoch_464_loss_24.pth'\n",
    "\n",
    "loaded_model = load_model_from_file(num_classes, model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da406f4-cc94-4ede-a4ca-8a50a00e7973",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedTestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transforms=None):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        self.annotations_list = sorted(os.listdir(os.path.join(root, 'annotations_val')))\n",
    "        self.imgs = sorted(glob.glob(os.path.join(root,'val' , '*.png')))\n",
    "\n",
    "    def __getitem__(self, indx):\n",
    "        \n",
    "        # load annotation\n",
    "        filename, boxes, labels = parse_xml(os.path.join(self.root, 'annotations_val', self.annotations_list[indx]))\n",
    "        \n",
    "        \n",
    "        # load image\n",
    "        img_path = os.path.join(self.root, 'val', filename)\n",
    "        \n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        num_objs = boxes.shape[0]\n",
    "        \n",
    "        #classes\n",
    "        labels = torch.tensor(labels, dtype=torch.int64)\n",
    "        image_id = torch.tensor([indx])\n",
    "        \n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "        \n",
    "        target = {}\n",
    "        target['boxes'] = boxes\n",
    "        target['labels'] = labels\n",
    "        target['image_id'] = image_id\n",
    "        target['area'] = area\n",
    "        target['iscrowd'] =iscrowd\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "        return img, target\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c53dfd5-ad0c-47ab-8850-5d43b65cb1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "import os, glob\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "class MaskedFaceTestDataset(Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        super(MaskedFaceTestDataset, self).__init__()\n",
    "        self.imgs = sorted(glob.glob(os.path.join(root, '*.png')))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.imgs[index]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f46a05d-97fa-415c-8f46-cdba02b736f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def get_transform(train):\n",
    "    #Transform of the images.\n",
    "    transform_list= []\n",
    "    transform_list.append(v2.PILToTensor())\n",
    "    transform_list.append(v2.ConvertImageDtype(torch.float))\n",
    "    \n",
    "    if train:\n",
    "        # during training, randomly flip the training images\n",
    "        # and ground-truth for data augmentation\n",
    "        # This configuration best for training.\n",
    "        transform_list.append(v2.RandomHorizontalFlip(0.5))\n",
    "        # Random affine not working well at lower epochs.\n",
    "        transform_list.append(v2.RandomAffine(5))   \n",
    "        # Image resize transformation does not work well for this training.\n",
    "        transform_list.append(v2.Normalize((0.5, 0.5, 0.5),(0.5, 0.5, 0.5)))\n",
    "        \n",
    "    transform_object = v2.Compose(transform_list)\n",
    "    \n",
    "    return transform_object\n",
    " '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b0f417-1045-4b75-9bdd-4e3c883e94e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform(train):\n",
    "    #Transform of the images.\n",
    "    transform_list= []\n",
    "    \n",
    "    transform_list.append(v2.PILToTensor())\n",
    "    transform_list.append(v2.ConvertImageDtype(torch.float))\n",
    "    \n",
    "    if train:\n",
    "        # during training, randomly flip the training images\n",
    "        # and ground-truth for data augmentation\n",
    "        # This configuration best for training.\n",
    "        # transform_list.append(v2.Resize((256,256), antialias=None))\n",
    "        # transform_list.append(v2.RandomResizedCrop(size=(224, 224), antialias=True))\n",
    "        # transform_list.append(v2.ColorJitter())\n",
    "        transform_list.append(v2.RandomHorizontalFlip(0.5))\n",
    "        # Random affine not working well at lower epochs.\n",
    "        # transform_list.append(v2.RandomAffine(15))   \n",
    "        # Image resize transformation does not work well for this training.\n",
    "        #transform_list.append(v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]))\n",
    "    #else:\n",
    "        # transform_list.append(v2.Resize((256,256), antialias=None))\n",
    "        # transform_list.append(v2.RandomResizedCrop(size=(224, 224), antialias=True))\n",
    "        #transform_list.append(v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]))\n",
    "    \n",
    "    transform_object = v2.Compose(transform_list)\n",
    "    \n",
    "    return transform_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aca4839-fad4-470e-ad4d-8cd8cfe29f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a7618a-59ba-48d6-a5bd-c91a52f83353",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = MaskedTestDataset(root='MaskedFace', transforms=get_transform(train=False))\n",
    "print(len(val_dataset))\n",
    "# data loader\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=True, num_workers=12, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377e76fa-a4f7-42f1-9842-e03e2e038536",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = MaskedFaceTestDataset(root='MaskedFaceTest', transform=get_transform(train=False))\n",
    "#test_dataset = MaskedFaceTestDataset(root='MaskedFaceTest')\n",
    "print(len(test_dataset))\n",
    "# data loader\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=True, num_workers=12, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8205174-11a6-46c8-9bb2-f7a35ba8bec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_model = loaded_model.cpu()\n",
    "infer_model.eval()\n",
    "print(\"Evaluation Mode of Model:-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be24371e-1229-451c-a9b5-2f09ca3ea542",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_str2num = {'with_mask': 1,'without_mask': 2,'mask_weared_incorrect': 3}\n",
    "class_num2str = {v: k for k, v in class_str2num.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d71035-a8a6-45ed-82b1-297b15fc6c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "def parse_xml(xml_file):\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "    bboxes = []\n",
    "    labels = []\n",
    "    filename = root.find('filename').text\n",
    "    for boxes in root.iter('object'):\n",
    "        ymin, xmin, ymax, xmax = None, None, None, None\n",
    "        ymin = int(boxes.find(\"bndbox/ymin\").text)\n",
    "        xmin = int(boxes.find(\"bndbox/xmin\").text)\n",
    "        ymax = int(boxes.find(\"bndbox/ymax\").text)\n",
    "        xmax = int(boxes.find(\"bndbox/xmax\").text)\n",
    "        box = [xmin, ymin, xmax, ymax]\n",
    "        bboxes.append(box)\n",
    "        labels.append(int(class_str2num[boxes.find(\"name\").text]))\n",
    "    return filename, bboxes, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2bcc02-c23d-4ad2-8432-0a282787a72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from collections import Counter\n",
    "print(\"Class numbers\")\n",
    "print(class_num2str)\n",
    "\n",
    "def calculate_MAPE(test_dataset, confidence_score, model):\n",
    "    \n",
    "    class_score = 0\n",
    "    class_score_ind=[]\n",
    "    \n",
    "    for i in range(len(test_dataset)):\n",
    "        \n",
    "        img, _ = test_dataset[i]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Prediction for the particular image.\n",
    "            prediction = model([img])\n",
    "        \n",
    "        groundtruth_labels = test_dataset[i][1]['labels'].numpy()\n",
    "        confident_prediction_labels=[]\n",
    "        \n",
    "        # For all the predicted boxes check the labels and confidence scores.\n",
    "        for element in range(len(prediction[0]['boxes'])):\n",
    "\n",
    "            #print(\"Prediction Label of each box:-\")\n",
    "            ele_pred_label = prediction[0]['labels'][element].cpu().item()\n",
    "            #print(ele_pred_label)\n",
    "            \n",
    "            score = np.round(prediction[0]['scores'][element].cpu().numpy(), decimals= 4)\n",
    "            # if the score is greater than confidence score.\n",
    "            if score > confidence_score:\n",
    "               # print(score)\n",
    "               # if  the score is greater than the confidence score\n",
    "               confident_prediction_labels.append(ele_pred_label)\n",
    "        \n",
    "        #print(\"The confident prediction labels:-\")\n",
    "        #print(confident_prediction_labels)\n",
    "        C_grd = Counter(groundtruth_labels)\n",
    "        #print(C_grd)\n",
    "        C_pred = Counter(confident_prediction_labels)   \n",
    "        #print(C_pred)\n",
    "        \n",
    "        # Classes in the Object detection:-\n",
    "        # print(class_num2str)\n",
    "        class_score=0\n",
    "        # Class keys.\n",
    "        for k in class_num2str.keys():\n",
    "            # print(\"Class Labels:-\")\n",
    "            # print(k)\n",
    "            At = C_grd[k]\n",
    "            Pt = C_pred[k] \n",
    "            class_score += abs(At-Pt)/max(At,1)\n",
    "        \n",
    "        # Find the average class score of all the classes.\n",
    "        class_score_ind.append(class_score/(len(class_num2str.keys())+1))\n",
    "    \n",
    "    sum_scores = sum(class_score_ind)\n",
    "    #print(\"Scores_sum\")\n",
    "    #print(sum_scores)\n",
    "    #print(\"Class Scores:-\")\n",
    "    #print(class_score)  \n",
    "    #print(\"Class_score_List:-\")\n",
    "    #print(class_score_ind)\n",
    "    #print(\"length of class scores:-\")\n",
    "    #print(len(class_score_ind))\n",
    "    #print(\"length of test dataset\")\n",
    "    #print(len(test_dataset))\n",
    "    Avg_MAPE = (sum_scores/len(test_dataset)) * 100\n",
    "    print(\"Avg MAPE Score on the dataset\")\n",
    "    print(Avg_MAPE)\n",
    "    \n",
    "confidence_score = 0.7\n",
    "calculate_MAPE(val_dataset, confidence_score, infer_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11a951e-9d21-4208-8010-4ae6f3e3a30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count masked faces\n",
    "def count_masks_val(input_dataset, model, confidence_score=0.5):\n",
    "    \n",
    "    len_dataset = len(input_dataset)\n",
    "    \n",
    "    inference_results_matrix = np.zeros((len_dataset,3))\n",
    "\n",
    "    for i in range(len(input_dataset)):\n",
    "        img, _ = input_dataset[i]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            prediction = model([img])\n",
    "    \n",
    "        for element in range(len(prediction[0]['boxes'])):\n",
    "            label = prediction[0]['labels'][element].cpu().item()\n",
    "            score = np.round(prediction[0]['scores'][element].cpu().numpy(), decimals= 4)\n",
    "            if score > confidence_score:\n",
    "               inference_results_matrix[i][label-1] += 1 \n",
    "            \n",
    "    return inference_results_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a543c41a-33b9-4ec7-aa3f-67679e41e3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count masked faces\n",
    "def count_masks_test(input_dataset, model, confidence_score=0.5):\n",
    "    \n",
    "    len_dataset = len(input_dataset)\n",
    "    \n",
    "    inference_results_matrix = np.zeros((len_dataset,3))\n",
    "\n",
    "    for i in range(len(input_dataset)):\n",
    "        img = input_dataset[i]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            prediction = model([img])\n",
    "    \n",
    "        for element in range(len(prediction[0]['boxes'])):\n",
    "            label = prediction[0]['labels'][element].cpu().item()\n",
    "            score = np.round(prediction[0]['scores'][element].cpu().numpy(), decimals= 4)\n",
    "            if score > confidence_score:\n",
    "               inference_results_matrix[i][label-1] += 1 \n",
    "            \n",
    "    return inference_results_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0aa714f-959b-4d0e-a3e9-22d3dd78790a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "confidence_score=0.6\n",
    "inf_results = count_masks_val(val_dataset, infer_model, confidence_score)\n",
    "print(inf_results.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fd00a0-79c5-45e4-847f-87c61314449f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "confidence_score=0.7\n",
    "inf_results1 = count_masks_test(test_dataset, infer_model, confidence_score)\n",
    "print(inf_results1.shape)\n",
    "print(inf_results1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98d73cc-39c0-48d7-a38d-7e421f900c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (10.0, 10.0)\n",
    "plt.rcParams['figure.dpi'] = 72\n",
    "\n",
    "# Iterate the test dataset.\n",
    "for i in range(len(test_dataset)):\n",
    "    img = test_dataset[i]\n",
    "    \n",
    "    #put the model in evaluation mode\n",
    "    with torch.no_grad():\n",
    "        prediction = infer_model([img])    \n",
    "    image = Image.fromarray(img.mul(255).permute(1, 2, 0).byte().numpy())\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    \n",
    "    # Plot each prediction boxes.\n",
    "    for element in range(len(prediction[0]['boxes'])):\n",
    "        box = prediction[0]['boxes'][element].cpu().numpy()\n",
    "        label = prediction[0]['labels'][element].cpu().item()\n",
    "        score = np.round(prediction[0]['scores'][element].cpu().numpy(), decimals= 4)\n",
    "        if score > 0.7:\n",
    "            draw.rectangle([(box[0], box[1]), (box[2], box[3])], outline ='blue', width =3)\n",
    "            draw.text((box[0], box[1]), text = class_num2str[label] + ', ' + str(score))\n",
    "    plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac97edd-772e-4f65-a6a2-f1691143df4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (10.0, 10.0)\n",
    "plt.rcParams['figure.dpi'] = 72\n",
    "\n",
    "for i in range(len(val_dataset)):\n",
    "    img, _ = val_dataset[i]\n",
    "    label_boxes = np.array(val_dataset[i][1]['boxes'])\n",
    "    #put the model in evaluation mode\n",
    "    with torch.no_grad():\n",
    "        prediction = infer_model([img])\n",
    "    image = Image.fromarray(img.mul(255).permute(1, 2, 0).byte().numpy())\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    # draw groundtruth\n",
    "    for elem in range(len(label_boxes)):\n",
    "        draw.rectangle([(label_boxes[elem][0], label_boxes[elem][1]), (label_boxes[elem][2], label_boxes[elem][3])], outline =\"yellow\", width =3)\n",
    "    for element in range(len(prediction[0]['boxes'])):\n",
    "        box = prediction[0]['boxes'][element].cpu().numpy()\n",
    "        label = prediction[0]['labels'][element].cpu().item()\n",
    "        score = np.round(prediction[0]['scores'][element].cpu().numpy(), decimals= 4)\n",
    "        if score > 0.6:\n",
    "            draw.rectangle([(box[0], box[1]), (box[2], box[3])], outline ='blue', width =3)\n",
    "            draw.text((box[0], box[1]), text = class_num2str[label] + ', ' + str(score))\n",
    "    plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8f07ee-5c8e-4d57-9d51-6a564f0c95a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
